services:
  llama:
    image: 'pytorch/pytorch'
    platform: linux/amd64
    ports:
      - '5000'
  frontend:
    image: 'node:18.19'
    ports:
      - '3000:3000'
